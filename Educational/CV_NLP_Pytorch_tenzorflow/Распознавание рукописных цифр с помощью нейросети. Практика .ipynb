{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d89d32",
   "metadata": {
    "id": "a1d89d32"
   },
   "source": [
    "## Цель ноутбука\n",
    "\n",
    "Построить и обучить модель для распознавания рукописных цифр на базе датасета [MNIST](http://yann.lecun.com/exdb/mnist/), используя нейронные сети.\n",
    "\n",
    "На примере этой задачи мы рассмотрим процесс обучения нейронных сетей, необходимые его составляющие, а также сможем сравнить новый подход со знакомым методом классического машинного обучения.  \n",
    "\n",
    "Будем использовать фреймворк [PyTorch](https://pytorch.org).\n",
    "\n",
    "### 1. Устанавливаем и импортируем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b12c9",
   "metadata": {
    "id": "312b12c9"
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install torch torchvision\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0705b695",
   "metadata": {
    "id": "0705b695"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7888f61a",
   "metadata": {
    "id": "7888f61a"
   },
   "source": [
    "### 2. Подготавливаем данные\n",
    "\n",
    "PyTorch предлагает свою версию датасета MNIST. Он возвращает готовый экземпляр класса [`torch.utils.data.Dataset`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). Каждый семпл датасета — это пара (изображение, лейбл), где изображение имеет размер 28 × 28 и является объектом класса [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html), а лейбл — число от 0 до 9, соответствующее цифре на изображении.\n",
    "\n",
    "Так как модели работают с тензорами, мы делаем из `PIL.Image` `torch.Tensor` методом `ToTensor()` из `torchvision.transforms`. Кроме этого, мы центрируем и нормируем данные, чтобы на вход модели приходили числа от −1 до 1.\n",
    "\n",
    "Датасет мы оборачиваем в `Dataloader`, чтобы получить итерируемый объект и позаботиться о группировке семплов в батчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ed1754",
   "metadata": {
    "id": "81ed1754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 9912422/9912422 [00:03<00:00, 2539578.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 240039.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 1947697.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "\n",
      "Train: 60000\n",
      "Valid: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MNIST('data/', train=True, download=True)\n",
    "val_dataset = MNIST('data/', train=False)\n",
    "\n",
    "print('Train:', len(train_dataset))\n",
    "print('Valid:', len(val_dataset))\n",
    "\n",
    "for i in range(10):\n",
    "    img, lbl = train_dataset[i]\n",
    "    print(lbl, img.size)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316d1a64-8417-4dc4-89d1-b201a8ca1565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28>, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592c1ae",
   "metadata": {
    "id": "5592c1ae",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_dataset = MNIST('data/', train=True, download=True, transform=transform)\n",
    "val_dataset = MNIST('data/', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426f909",
   "metadata": {
    "id": "2426f909"
   },
   "source": [
    "### 3. Строим модель\n",
    "\n",
    "Любой класс модели должен наследоваться от `torch.nn.Module` и иметь метод `forward()` для вызова модели. Первым делом мы попробуем написать логистическую регрессию для мультиклассовой классификации, она же Softmax-регрессия.\n",
    "\n",
    "Чтобы правильно написать метод `forward()`, нужно сразу понять, с каким лоссом мы будем учить нашу модель. Удобный вариант — кросс-энтропия [`torch.nn.CrossEntropyLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "\n",
    "$$H(p,q) = -\\sum_x p(x)\\log q(x)$$\n",
    "\n",
    "$$L = - \\frac{1}{N} \\sum_n \\left( 1 * \\log \\frac{\\exp{x_{n,y_n}}}{\\sum_c \\exp{x_{n,c}}} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a8848-34a9-4fc4-b8eb-c4b277200b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ede9d",
   "metadata": {
    "id": "2e0ede9d"
   },
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fccb7df",
   "metadata": {
    "id": "0fccb7df"
   },
   "source": [
    "### 4. Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52e96e",
   "metadata": {
    "id": "5c52e96e"
   },
   "outputs": [],
   "source": [
    "model = LogReg(in_features=28*28, n_classes=10)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "n_epoch = 50\n",
    "val_fre = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epoch):\n",
    "    loss_sum = 0\n",
    "    for step, (data, target) in enumerate(train_loader):\n",
    "        data = data.flatten(start_dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_f(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "    print(f'Epoch: {epoch} \\tLoss: {loss_sum / (step + 1):.6f}')\n",
    "\n",
    "    if (epoch+1) % val_fre == 0:\n",
    "        model.eval()\n",
    "        loss_sum = 0\n",
    "        correct = 0\n",
    "        for step, (data, target) in enumerate(val_loader):\n",
    "            data = data.flatten(start_dim=1)\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "                loss = loss_f(output, target)\n",
    "            loss_sum += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        acc = correct / len(val_loader.dataset)\n",
    "        print(f'Val Loss: {loss_sum / (step + 1):.6f} \\tAccuracy: {acc}')\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da205c",
   "metadata": {
    "id": "a0da205c"
   },
   "source": [
    "### 5. Сохраняем (и загружаем) модель\n",
    "\n",
    "Если обучение модели не завершено, то [аналогичным образом](https://pytorch.org/tutorials/beginner/saving_loading_models.html) можно сохранить и оптимизатор, и scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e841e4",
   "metadata": {
    "id": "71e841e4"
   },
   "outputs": [],
   "source": [
    "os.makedirs('checkpoints/', exist_ok=True)\n",
    "torch.save(model.state_dict(), 'checkpoints/logreg.pth')\n",
    "\n",
    "model = LogReg(in_features=28*28, n_classes=10)\n",
    "model.load_state_dict(torch.load('checkpoints/logreg.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da44f6c",
   "metadata": {
    "id": "6da44f6c"
   },
   "source": [
    "### 6. Рубрика «Эксперименты»\n",
    "\n",
    "Упакуем обучение и валидацию в функции и попробуем заменить линейный слой на двухслойный перцептрон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb89130",
   "metadata": {
    "id": "8bb89130"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_f, train_loader, val_loader, n_epoch, val_fre):\n",
    "    model.train()\n",
    "    for epoch in range(n_epoch):\n",
    "        loss_sum = 0\n",
    "        for step, (data, target) in enumerate(train_loader):\n",
    "            data = data.flatten(start_dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_f(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        print(f'Epoch: {epoch} \\tLoss: {loss_sum / (step + 1):.6f}')\n",
    "\n",
    "        if epoch % val_fre == 0:\n",
    "            validate(model, val_loader)\n",
    "\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    for step, (data, target) in enumerate(val_loader):\n",
    "        data = data.flatten(start_dim=1)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            loss = loss_f(output, target)\n",
    "        loss_sum += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    acc = correct / len(val_loader.dataset)\n",
    "    print(f'Val Loss: {loss_sum / (step + 1):.6f} \\tAccuracy: {acc}')\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32372b9",
   "metadata": {
    "id": "e32372b9"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hid_features, n_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hid_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hid_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8e034",
   "metadata": {
    "id": "b3d8e034"
   },
   "outputs": [],
   "source": [
    "model_mlp = MLP(in_features=28*28, hid_features=1024, n_classes=10)\n",
    "print(model_mlp)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_mlp.parameters(), lr=1e-1)\n",
    "\n",
    "n_epoch = 20\n",
    "val_fre = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333fe697",
   "metadata": {
    "id": "333fe697"
   },
   "outputs": [],
   "source": [
    "train(model_mlp, optimizer, loss_f, train_loader, val_loader, n_epoch, val_fre)\n",
    "validate(model_mlp, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc43e61",
   "metadata": {
    "id": "3dc43e61"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
