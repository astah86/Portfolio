{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p92rzC-2D71g"
   },
   "source": [
    "# Инфраструктура для моделей машинного обучения. Практическая работа\n",
    "\n",
    "# Цель практической работы\n",
    "\n",
    "Потренироваться в использовании библиотек PySpark SQL и PySpark ML для предобработки данных и обучения моделей.\n",
    "\n",
    "# Что входит в практическую работу\n",
    "\n",
    "1. Инициализация спарк-сессии.\n",
    "2. Загрузка данных.\n",
    "3. Ознакомление с данными.\n",
    "4. Преобразование типов столбцов.\n",
    "5. Очистка данных.\n",
    "6. Feature-инжиниринг.\n",
    "7. Векторизация фичей.\n",
    "8. Создание и обучение модели.\n",
    "9. Выбор лучшей модели.\n",
    "10. Обратная связь.\n",
    "\n",
    "\n",
    "# Что оценивается \n",
    "\n",
    "- Пройдены все этапы работы.\n",
    "- Спарк-сессия успешно запущена.\n",
    "- Данные прочитаны.\n",
    "- Все колонки с числовыми значениями преобразованы в числовые типы данных (Int или Double).\n",
    "- Отфильтрованы все строки с Null-значениями.\n",
    "- Созданы новые фичи.\n",
    "- Все категориальные колонки преобразованы в числовой вид, выполнены все этапы векторизации признаков.\n",
    "- Выборка разделена на обучающую и тестовую.\n",
    "- Создано три объекта: модель, сетка гиперпараметров и evaluator.\n",
    "- Создан объект класса CrossValidator и обучен на обучающей выборке.\n",
    "- Выбрана лучшая модель, посчитана метрика качества лучшей модели.\n",
    "\n",
    "\n",
    "# Задача\n",
    "\n",
    "Используя данные о клиентах телекоммуникационной компании, обучите модель, предсказывающую их отток.\n",
    "\n",
    "Описание данных, с которыми вы будете работать:\n",
    "\n",
    "* **CustomerID**: ID клиента.\n",
    "* **Gender**: пол клиента.\n",
    "* **SeniorCitizen**: пенсионер ли клиент (1 — да, 0 — нет).\n",
    "* **Partner**: есть у клиента партнёр (жена, муж) или нет (Yes/No).\n",
    "* **Dependents**: есть ли у клиента инждивенцы, например дети (Yes/No).\n",
    "* **Tenure**: как много месяцев клиент оставался в компании.\n",
    "* **PhoneService**: подключена ли у клиента телефонная служба (Yes/No).\n",
    "* **MultipleLines**: подключено ли несколько телефонных линий (Yes, No, No phone service).\n",
    "* **InternetService**: интернет-провайдер клиента (DSL, Fiber optic, No).\n",
    "* **OnlineSecurity**: подключена ли у клиента услуга онлайн-безопасности (Yes, No, No internet service)\n",
    "* **OnlineBackup**: подключена ли услуга резервного копирования онлайн (Yes, No, No internet service).\n",
    "* **DeviceProtection**: подключена ли услуга защиты устройства (Yes, No, No internet service)\n",
    "* **TechSupport**: есть ли у клиента техническая поддержка (Yes, No, No internet service).\n",
    "* **StreamingTV**: подключена ли услуга потокового телевидения (Yes, No, No internet service).\n",
    "* **StreamingMovies**: подключена ли услуга стримингового воспроизведения фильмов (Yes, No, No internet service).\n",
    "* **Contract**: тип контракта клиента (Month-to-month, One year, Two year).\n",
    "* **PaperlessBilling**: есть ли безбумажный счёт.\n",
    "* **PaymentMethod**: способ оплаты услуг (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)).\n",
    "* **MonthlyCharges**: сумма, которая списывается ежемесячно.\n",
    "* **TotalCharges**: сумма, списанная за всё время.\n",
    "* **Churn**: ушёл ли клиент (Yes/No). Это целевая переменная, которую нужно предсказать.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZmr6VeYvLKB"
   },
   "source": [
    "# 1. Инициализация спарк-сессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO2v2cOjyZBN"
   },
   "source": [
    "Инициализируйте спарк-сессию.\n",
    "\n",
    "Эта ячейка нужна для того, чтобы заргузить необходимые библиотеки и настроить окружение Google Colab для работы со Spark.\n",
    "\n",
    "Просто запустите её перед выполением задания :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UuUc1cxHuymb",
    "outputId": "b14a1c63-36ca-4413-dc2d-59ccbe9799ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark --quiet\n",
    "!pip install -U -q PyDrive --quiet \n",
    "!apt install openjdk-8-jdk-headless &> /dev/null\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "\n",
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "get_ipython().system_raw('./ngrok http 4050 &')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Msi-1\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - findspark\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.5.7   |       h56e8100_0         145 KB  conda-forge\n",
      "    certifi-2023.5.7           |     pyhd8ed1ab_0         149 KB  conda-forge\n",
      "    findspark-2.0.1            |     pyhd8ed1ab_0           8 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         302 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  findspark          conda-forge/noarch::findspark-2.0.1-pyhd8ed1ab_0 None\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2023.5.7-h56e8100_0 None\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/win-64::certifi-2023.5.7-py~ --> conda-forge/noarch::certifi-2023.5.7-pyhd8ed1ab_0 None\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "findspark-2.0.1      | 8 KB      |            |   0% \n",
      "findspark-2.0.1      | 8 KB      | ########## | 100% \n",
      "findspark-2.0.1      | 8 KB      | ########## | 100% \n",
      "\n",
      "certifi-2023.5.7     | 149 KB    |            |   0% \n",
      "certifi-2023.5.7     | 149 KB    | ########## | 100% \n",
      "certifi-2023.5.7     | 149 KB    | ########## | 100% \n",
      "\n",
      "ca-certificates-2023 | 145 KB    |            |   0% \n",
      "ca-certificates-2023 | 145 KB    | ########## | 100% \n",
      "ca-certificates-2023 | 145 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - openjdk-8-jdk-headless\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/win-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/win-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "  - https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "  - https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - openjdk-8-jdk-headless\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/win-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/win-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "  - https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "  - https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install openjdk-8-jdk-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Msi-1\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pyspark\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    abseil-cpp-20211102.0      |       hd77b12b_0         1.7 MB\n",
      "    arrow-cpp-11.0.0           |   py39h3577439_0         7.2 MB\n",
      "    aws-c-common-0.4.57        |       ha925a31_1         147 KB\n",
      "    aws-c-event-stream-0.1.6   |       hd77b12b_5          26 KB\n",
      "    aws-checksums-0.1.9        |       ha925a31_0          50 KB\n",
      "    aws-sdk-cpp-1.8.185        |       hd77b12b_0         2.5 MB\n",
      "    boost-cpp-1.73.0           |      h2bbff1b_12          16 KB\n",
      "    c-ares-1.19.0              |       h2bbff1b_0         117 KB\n",
      "    gflags-2.2.2               |       ha925a31_0         233 KB\n",
      "    glog-0.5.0                 |       hd77b12b_0          85 KB\n",
      "    libboost-1.73.0            |      h6c2663c_12        20.1 MB\n",
      "    libprotobuf-3.20.3         |       h23ce68f_0         2.2 MB\n",
      "    libthrift-0.15.0           |       he1d8c1a_0         675 KB\n",
      "    orc-1.7.4                  |       h623e30f_1         823 KB\n",
      "    py4j-0.10.9.3              |   py39haa95532_0         252 KB\n",
      "    pyarrow-11.0.0             |   py39haa45b5f_0         3.2 MB\n",
      "    pyspark-3.2.1              |   py39haa95532_0       263.9 MB\n",
      "    re2-2022.04.01             |       hd77b12b_0         374 KB\n",
      "    utf8proc-2.6.1             |       h2bbff1b_0         312 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       303.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  abseil-cpp         pkgs/main/win-64::abseil-cpp-20211102.0-hd77b12b_0 None\n",
      "  arrow-cpp          pkgs/main/win-64::arrow-cpp-11.0.0-py39h3577439_0 None\n",
      "  aws-c-common       pkgs/main/win-64::aws-c-common-0.4.57-ha925a31_1 None\n",
      "  aws-c-event-stream pkgs/main/win-64::aws-c-event-stream-0.1.6-hd77b12b_5 None\n",
      "  aws-checksums      pkgs/main/win-64::aws-checksums-0.1.9-ha925a31_0 None\n",
      "  aws-sdk-cpp        pkgs/main/win-64::aws-sdk-cpp-1.8.185-hd77b12b_0 None\n",
      "  boost-cpp          pkgs/main/win-64::boost-cpp-1.73.0-h2bbff1b_12 None\n",
      "  c-ares             pkgs/main/win-64::c-ares-1.19.0-h2bbff1b_0 None\n",
      "  gflags             pkgs/main/win-64::gflags-2.2.2-ha925a31_0 None\n",
      "  glog               pkgs/main/win-64::glog-0.5.0-hd77b12b_0 None\n",
      "  libboost           pkgs/main/win-64::libboost-1.73.0-h6c2663c_12 None\n",
      "  libprotobuf        pkgs/main/win-64::libprotobuf-3.20.3-h23ce68f_0 None\n",
      "  libthrift          pkgs/main/win-64::libthrift-0.15.0-he1d8c1a_0 None\n",
      "  orc                pkgs/main/win-64::orc-1.7.4-h623e30f_1 None\n",
      "  py4j               pkgs/main/win-64::py4j-0.10.9.3-py39haa95532_0 None\n",
      "  pyarrow            pkgs/main/win-64::pyarrow-11.0.0-py39haa45b5f_0 None\n",
      "  pyspark            pkgs/main/win-64::pyspark-3.2.1-py39haa95532_0 None\n",
      "  re2                pkgs/main/win-64::re2-2022.04.01-hd77b12b_0 None\n",
      "  utf8proc           pkgs/main/win-64::utf8proc-2.6.1-h2bbff1b_0 None\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  lz4-c                                    1.9.3-h2bbff1b_1 --> 1.9.4-h2bbff1b_0 None\n",
      "  zlib                                    1.2.12-h8cc25b3_3 --> 1.2.13-h8cc25b3_0 None\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "pyarrow-11.0.0       | 3.2 MB    |            |   0% \n",
      "pyarrow-11.0.0       | 3.2 MB    |            |   0% \n",
      "pyarrow-11.0.0       | 3.2 MB    | #7         |  17% \n",
      "pyarrow-11.0.0       | 3.2 MB    | ####2      |  43% \n",
      "pyarrow-11.0.0       | 3.2 MB    | ######6    |  66% \n",
      "pyarrow-11.0.0       | 3.2 MB    | ########8  |  89% \n",
      "pyarrow-11.0.0       | 3.2 MB    | ########## | 100% \n",
      "\n",
      "utf8proc-2.6.1       | 312 KB    |            |   0% \n",
      "utf8proc-2.6.1       | 312 KB    | ########## | 100% \n",
      "utf8proc-2.6.1       | 312 KB    | ########## | 100% \n",
      "\n",
      "re2-2022.04.01       | 374 KB    |            |   0% \n",
      "re2-2022.04.01       | 374 KB    | ########## | 100% \n",
      "re2-2022.04.01       | 374 KB    | ########## | 100% \n",
      "\n",
      "abseil-cpp-20211102. | 1.7 MB    |            |   0% \n",
      "abseil-cpp-20211102. | 1.7 MB    | ##4        |  25% \n",
      "abseil-cpp-20211102. | 1.7 MB    | ########1  |  82% \n",
      "abseil-cpp-20211102. | 1.7 MB    | ########## | 100% \n",
      "\n",
      "libboost-1.73.0      | 20.1 MB   |            |   0% \n",
      "libboost-1.73.0      | 20.1 MB   | 3          |   3% \n",
      "libboost-1.73.0      | 20.1 MB   | 8          |   9% \n",
      "libboost-1.73.0      | 20.1 MB   | #4         |  14% \n",
      "libboost-1.73.0      | 20.1 MB   | #9         |  20% \n",
      "libboost-1.73.0      | 20.1 MB   | ##5        |  25% \n",
      "libboost-1.73.0      | 20.1 MB   | ##9        |  30% \n",
      "libboost-1.73.0      | 20.1 MB   | ###7       |  37% \n",
      "libboost-1.73.0      | 20.1 MB   | ####3      |  44% \n",
      "libboost-1.73.0      | 20.1 MB   | #####      |  50% \n",
      "libboost-1.73.0      | 20.1 MB   | #####5     |  56% \n",
      "libboost-1.73.0      | 20.1 MB   | ######1    |  61% \n",
      "libboost-1.73.0      | 20.1 MB   | ######9    |  69% \n",
      "libboost-1.73.0      | 20.1 MB   | #######5   |  75% \n",
      "libboost-1.73.0      | 20.1 MB   | ########1  |  81% \n",
      "libboost-1.73.0      | 20.1 MB   | ########7  |  88% \n",
      "libboost-1.73.0      | 20.1 MB   | #########5 |  95% \n",
      "libboost-1.73.0      | 20.1 MB   | ########## | 100% \n",
      "\n",
      "boost-cpp-1.73.0     | 16 KB     |            |   0% \n",
      "boost-cpp-1.73.0     | 16 KB     | ########## | 100% \n",
      "boost-cpp-1.73.0     | 16 KB     | ########## | 100% \n",
      "\n",
      "libthrift-0.15.0     | 675 KB    |            |   0% \n",
      "libthrift-0.15.0     | 675 KB    | #########2 |  92% \n",
      "libthrift-0.15.0     | 675 KB    | ########## | 100% \n",
      "\n",
      "py4j-0.10.9.3        | 252 KB    |            |   0% \n",
      "py4j-0.10.9.3        | 252 KB    | 6          |   6% \n",
      "py4j-0.10.9.3        | 252 KB    | ########## | 100% \n",
      "py4j-0.10.9.3        | 252 KB    | ########## | 100% \n",
      "\n",
      "aws-sdk-cpp-1.8.185  | 2.5 MB    |            |   0% \n",
      "aws-sdk-cpp-1.8.185  | 2.5 MB    | ##8        |  28% \n",
      "aws-sdk-cpp-1.8.185  | 2.5 MB    | #######5   |  75% \n",
      "aws-sdk-cpp-1.8.185  | 2.5 MB    | ########## | 100% \n",
      "\n",
      "glog-0.5.0           | 85 KB     |            |   0% \n",
      "glog-0.5.0           | 85 KB     | ########## | 100% \n",
      "glog-0.5.0           | 85 KB     | ########## | 100% \n",
      "\n",
      "libprotobuf-3.20.3   | 2.2 MB    |            |   0% \n",
      "libprotobuf-3.20.3   | 2.2 MB    | ###3       |  33% \n",
      "libprotobuf-3.20.3   | 2.2 MB    | ########2  |  83% \n",
      "libprotobuf-3.20.3   | 2.2 MB    | ########## | 100% \n",
      "\n",
      "orc-1.7.4            | 823 KB    |            |   0% \n",
      "orc-1.7.4            | 823 KB    | #######7   |  78% \n",
      "orc-1.7.4            | 823 KB    | ########## | 100% \n",
      "\n",
      "gflags-2.2.2         | 233 KB    |            |   0% \n",
      "gflags-2.2.2         | 233 KB    | ########## | 100% \n",
      "gflags-2.2.2         | 233 KB    | ########## | 100% \n",
      "\n",
      "pyspark-3.2.1        | 263.9 MB  |            |   0% \n",
      "pyspark-3.2.1        | 263.9 MB  |            |   0% \n",
      "pyspark-3.2.1        | 263.9 MB  |            |   0% \n",
      "pyspark-3.2.1        | 263.9 MB  |            |   0% \n",
      "pyspark-3.2.1        | 263.9 MB  |            |   0% \n",
      "pyspark-3.2.1        | 263.9 MB  |            |   0% \n",
      "pyspark-3.2.1        | 263.9 MB  |            |   1% \n",
      "pyspark-3.2.1        | 263.9 MB  |            |   1% \n",
      "pyspark-3.2.1        | 263.9 MB  | 1          |   1% \n",
      "pyspark-3.2.1        | 263.9 MB  | 1          |   2% \n",
      "pyspark-3.2.1        | 263.9 MB  | 2          |   2% \n",
      "pyspark-3.2.1        | 263.9 MB  | 2          |   3% \n",
      "pyspark-3.2.1        | 263.9 MB  | 3          |   3% \n",
      "pyspark-3.2.1        | 263.9 MB  | 3          |   4% \n",
      "pyspark-3.2.1        | 263.9 MB  | 4          |   4% \n",
      "pyspark-3.2.1        | 263.9 MB  | 4          |   5% \n",
      "pyspark-3.2.1        | 263.9 MB  | 5          |   5% \n",
      "pyspark-3.2.1        | 263.9 MB  | 5          |   6% \n",
      "pyspark-3.2.1        | 263.9 MB  | 6          |   6% \n",
      "pyspark-3.2.1        | 263.9 MB  | 6          |   7% \n",
      "pyspark-3.2.1        | 263.9 MB  | 7          |   7% \n",
      "pyspark-3.2.1        | 263.9 MB  | 7          |   8% \n",
      "pyspark-3.2.1        | 263.9 MB  | 8          |   8% \n",
      "pyspark-3.2.1        | 263.9 MB  | 8          |   9% \n",
      "pyspark-3.2.1        | 263.9 MB  | 9          |   9% \n",
      "pyspark-3.2.1        | 263.9 MB  | 9          |  10% \n",
      "pyspark-3.2.1        | 263.9 MB  | #          |  10% \n",
      "pyspark-3.2.1        | 263.9 MB  | #          |  10% \n",
      "pyspark-3.2.1        | 263.9 MB  | #          |  11% \n",
      "pyspark-3.2.1        | 263.9 MB  | #1         |  11% \n",
      "pyspark-3.2.1        | 263.9 MB  | #1         |  12% \n",
      "pyspark-3.2.1        | 263.9 MB  | #2         |  12% \n",
      "pyspark-3.2.1        | 263.9 MB  | #2         |  13% \n",
      "pyspark-3.2.1        | 263.9 MB  | #3         |  13% \n",
      "pyspark-3.2.1        | 263.9 MB  | #3         |  14% \n",
      "pyspark-3.2.1        | 263.9 MB  | #4         |  14% \n",
      "pyspark-3.2.1        | 263.9 MB  | #4         |  15% \n",
      "pyspark-3.2.1        | 263.9 MB  | #5         |  15% \n",
      "pyspark-3.2.1        | 263.9 MB  | #5         |  15% \n",
      "pyspark-3.2.1        | 263.9 MB  | #5         |  16% \n",
      "pyspark-3.2.1        | 263.9 MB  | #6         |  16% \n",
      "pyspark-3.2.1        | 263.9 MB  | #6         |  17% \n",
      "pyspark-3.2.1        | 263.9 MB  | #7         |  17% \n",
      "pyspark-3.2.1        | 263.9 MB  | #7         |  18% \n",
      "pyspark-3.2.1        | 263.9 MB  | #8         |  18% \n",
      "pyspark-3.2.1        | 263.9 MB  | #8         |  19% \n",
      "pyspark-3.2.1        | 263.9 MB  | #9         |  19% \n",
      "pyspark-3.2.1        | 263.9 MB  | #9         |  20% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##         |  20% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##         |  21% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##1        |  21% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##1        |  22% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##2        |  22% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##2        |  23% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##2        |  23% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##3        |  23% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##3        |  24% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##4        |  24% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##4        |  25% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##5        |  25% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##5        |  26% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##6        |  26% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##6        |  27% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##7        |  27% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##7        |  27% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##7        |  28% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##8        |  28% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##8        |  29% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##9        |  29% \n",
      "pyspark-3.2.1        | 263.9 MB  | ##9        |  30% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###        |  30% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###        |  31% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###1       |  31% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###1       |  32% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###2       |  32% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###2       |  32% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###2       |  33% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###3       |  33% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###3       |  34% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###4       |  34% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###4       |  35% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###5       |  35% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###5       |  36% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###6       |  36% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###6       |  37% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###6       |  37% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###7       |  38% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###8       |  38% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###8       |  38% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###8       |  39% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###9       |  39% \n",
      "pyspark-3.2.1        | 263.9 MB  | ###9       |  40% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####       |  40% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####       |  41% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####1      |  41% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####1      |  42% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####2      |  42% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####2      |  43% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####3      |  43% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####3      |  43% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####3      |  44% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####4      |  44% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####4      |  45% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####5      |  45% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####5      |  45% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####6      |  46% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####6      |  46% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####6      |  47% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####7      |  47% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####7      |  48% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####8      |  48% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####8      |  49% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####8      |  49% \n",
      "pyspark-3.2.1        | 263.9 MB  | ####9      |  49% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####      |  50% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####      |  50% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####      |  51% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####1     |  51% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####2     |  52% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####2     |  53% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####2     |  53% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####3     |  53% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####3     |  54% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####4     |  54% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####4     |  55% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####5     |  55% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####5     |  56% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####6     |  56% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####6     |  57% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####7     |  57% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####7     |  58% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####8     |  58% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####8     |  59% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####9     |  59% \n",
      "pyspark-3.2.1        | 263.9 MB  | #####9     |  60% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######     |  60% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######     |  60% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######     |  61% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######1    |  61% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######1    |  62% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######2    |  62% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######2    |  63% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######3    |  63% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######3    |  64% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######4    |  64% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######4    |  65% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######5    |  65% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######5    |  65% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######5    |  66% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######6    |  66% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######6    |  67% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######7    |  67% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######7    |  68% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######8    |  68% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######8    |  69% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######8    |  69% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######9    |  69% \n",
      "pyspark-3.2.1        | 263.9 MB  | ######9    |  70% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######    |  70% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######    |  71% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######    |  71% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######1   |  71% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######1   |  72% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######2   |  72% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######2   |  73% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######3   |  73% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######3   |  74% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######4   |  74% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######4   |  75% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######5   |  75% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######5   |  76% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######6   |  76% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######6   |  77% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######7   |  77% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######7   |  77% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######7   |  78% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######8   |  78% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######8   |  79% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######9   |  79% \n",
      "pyspark-3.2.1        | 263.9 MB  | #######9   |  80% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########   |  80% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########   |  80% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########1  |  81% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########1  |  81% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########1  |  82% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########2  |  82% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########2  |  83% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########3  |  83% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########3  |  84% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########4  |  84% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########4  |  85% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########5  |  85% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########5  |  86% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########6  |  86% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########6  |  87% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########7  |  87% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########7  |  87% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########7  |  88% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########8  |  89% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########9  |  89% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########9  |  90% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########9  |  90% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########  |  90% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########  |  91% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########1 |  91% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########1 |  92% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########2 |  92% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########2 |  93% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########3 |  93% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########3 |  94% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########4 |  94% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########4 |  94% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########4 |  95% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########5 |  95% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########5 |  96% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########6 |  96% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########6 |  97% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########7 |  97% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########7 |  98% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########8 |  98% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########8 |  99% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########9 |  99% \n",
      "pyspark-3.2.1        | 263.9 MB  | #########9 | 100% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########## | 100% \n",
      "pyspark-3.2.1        | 263.9 MB  | ########## | 100% \n",
      "\n",
      "aws-c-common-0.4.57  | 147 KB    |            |   0% \n",
      "aws-c-common-0.4.57  | 147 KB    | ########## | 100% \n",
      "aws-c-common-0.4.57  | 147 KB    | ########## | 100% \n",
      "\n",
      "arrow-cpp-11.0.0     | 7.2 MB    |            |   0% \n",
      "arrow-cpp-11.0.0     | 7.2 MB    | 9          |   9% \n",
      "arrow-cpp-11.0.0     | 7.2 MB    | ##8        |  29% \n",
      "arrow-cpp-11.0.0     | 7.2 MB    | ####3      |  43% \n",
      "arrow-cpp-11.0.0     | 7.2 MB    | ######     |  61% \n",
      "arrow-cpp-11.0.0     | 7.2 MB    | ########   |  81% \n",
      "arrow-cpp-11.0.0     | 7.2 MB    | #########6 |  97% \n",
      "arrow-cpp-11.0.0     | 7.2 MB    | ########## | 100% \n",
      "\n",
      "aws-checksums-0.1.9  | 50 KB     |            |   0% \n",
      "aws-checksums-0.1.9  | 50 KB     | ########## | 100% \n",
      "aws-checksums-0.1.9  | 50 KB     | ########## | 100% \n",
      "\n",
      "c-ares-1.19.0        | 117 KB    |            |   0% \n",
      "c-ares-1.19.0        | 117 KB    | ########## | 100% \n",
      "c-ares-1.19.0        | 117 KB    | ########## | 100% \n",
      "\n",
      "aws-c-event-stream-0 | 26 KB     |            |   0% \n",
      "aws-c-event-stream-0 | 26 KB     | ########## | 100% \n",
      "aws-c-event-stream-0 | 26 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n",
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!apt install openjdk-8-jdk-headless &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8Q743I78vA5_"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('PySpark_Tutorial')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "### Ваш код здесь ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e55O3aB5vo3k"
   },
   "source": [
    "# 2. Загрузка данных\n",
    "Загрузите данные, сохраните их в переменную типа sparkDataframe, используя метод read.csv (не забывайте про header и delimiter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "1GVOybUCvlXy"
   },
   "outputs": [],
   "source": [
    "### Ваш код здесь ###\n",
    "df = spark.read.option(\"header\",True).option(\"delimiter\",\",\").csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8amaWNGzDsf"
   },
   "source": [
    "# 3. Ознакомление с данными\n",
    "1. Выведите на экран первые несколько строк датафрейма.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPfkFFUoz19Y",
    "outputId": "38a7e925-7241-42c1-b490-018f6a6f2eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|customerID|gender|SeniorCitizen|Partner|Dependents|tenure|PhoneService|   MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|7590-VHVEG|Female|            0|    Yes|        No|     1|          No|No phone service|            DSL|            No|         Yes|              No|         No|         No|             No|Month-to-month|             Yes|    Electronic check|         29.85|       29.85|   No|\n",
      "|5575-GNVDE|  Male|            0|     No|        No|    34|         Yes|              No|            DSL|           Yes|          No|             Yes|         No|         No|             No|      One year|              No|        Mailed check|         56.95|      1889.5|   No|\n",
      "|3668-QPYBK|  Male|            0|     No|        No|     2|         Yes|              No|            DSL|           Yes|         Yes|              No|         No|         No|             No|Month-to-month|             Yes|        Mailed check|         53.85|      108.15|  Yes|\n",
      "|7795-CFOCW|  Male|            0|     No|        No|    45|          No|No phone service|            DSL|           Yes|          No|             Yes|        Yes|         No|             No|      One year|              No|Bank transfer (au...|          42.3|     1840.75|   No|\n",
      "|9237-HQITU|Female|            0|     No|        No|     2|         Yes|              No|    Fiber optic|            No|          No|              No|         No|         No|             No|Month-to-month|             Yes|    Electronic check|          70.7|      151.65|  Yes|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Ваш код здесь ###\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMgX25wr0JfV"
   },
   "source": [
    "\n",
    "2. Выведите общее количество строк датафрейма.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "ivgw172qz9o_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7043"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Ваш код здесь ###\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCAeIZFe0KyY"
   },
   "source": [
    "3. Выведите структуру (схему) датафрейма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "KmZY25zX0Fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: string (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: string (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: string (nullable = true)\n",
      " |-- TotalCharges: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Ваш код здесь ###\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFcKPAI_0cF7"
   },
   "source": [
    "# 4. Преобразование типов столбцов\n",
    "Преобразуйте тип столбцов у числовых признаков (Int — если признак целочисленный, Double — если признак не целочисленный). Сохраните преобразованный датафрейм в новую переменную.\n",
    "\n",
    "## Совет\n",
    "\n",
    "Если вам сложно выполнить это задание, изучите дополнительные материалы: [об операторе Cast](https://sparkbyexamples.com/pyspark/pyspark-cast-column-type/), [об операторе Select](https://sparkbyexamples.com/pyspark/select-columns-from-pyspark-dataframe/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "Kihtrqni0-Js"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "df1 = df.select(\n",
    "    col(\"gender\"),\n",
    "    col(\"SeniorCitizen\").cast(\"Int\"),\n",
    "    col(\"Partner\"),\n",
    "    col(\"Dependents\"),\n",
    "    col(\"tenure\").cast(\"Int\"),\n",
    "    col(\"PhoneService\"),\n",
    "    col(\"MultipleLines\"),\n",
    "    col(\"InternetService\"),\n",
    "    col(\"OnlineSecurity\"),\n",
    "    col(\"OnlineBackup\"),\n",
    "    col(\"DeviceProtection\"),\n",
    "    col(\"TechSupport\"),\n",
    "    col(\"StreamingTV\"),\n",
    "    col(\"StreamingMovies\"),\n",
    "    col(\"Contract\"),\n",
    "    col(\"PaperlessBilling\"),\n",
    "    col(\"PaymentMethod\"),\n",
    "    col(\"MonthlyCharges\").cast(\"Double\"),\n",
    "    col(\"TotalCharges\").cast(\"Double\"),\n",
    "    col(\"Churn\")\n",
    ")\n",
    "\n",
    "### Ваш код здесь ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hBBiIm350BD"
   },
   "source": [
    "# 5. Очистка данных\n",
    "Проверьте, есть ли в какой-либо колонке Null-значения. Для этого можно использовать your_dataframe.filter(col(\"colname\")).isNull()). \n",
    "\n",
    "Выведите на экран несколько строк с Null-значениями в одной из колонок.\n",
    "\n",
    "Сохраните очищенный от строк с Null-значениями датафрейм в новую переменную. Для фильтрации этих значений можно использовать метод .isNotNull().\n",
    "\n",
    "Колонок в датафрейме много, проверять каждую неудобно и долго. Подумайте, как упроситить эту работу, если использовать, например, перебор с циклом for.\n",
    "\n",
    "[Примеры использования операторов isNull() и isNotNull()](https://sparkbyexamples.com/pyspark/pyspark-isnull/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "A0FTVvpg6_iy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество нулевых значений в колонке gender: 0\n",
      "Количество нулевых значений в колонке SeniorCitizen: 0\n",
      "Количество нулевых значений в колонке Partner: 0\n",
      "Количество нулевых значений в колонке Dependents: 0\n",
      "Количество нулевых значений в колонке tenure: 0\n",
      "Количество нулевых значений в колонке PhoneService: 0\n",
      "Количество нулевых значений в колонке MultipleLines: 0\n",
      "Количество нулевых значений в колонке InternetService: 0\n",
      "Количество нулевых значений в колонке OnlineSecurity: 0\n",
      "Количество нулевых значений в колонке OnlineBackup: 0\n",
      "Количество нулевых значений в колонке DeviceProtection: 0\n",
      "Количество нулевых значений в колонке TechSupport: 0\n",
      "Количество нулевых значений в колонке StreamingTV: 0\n",
      "Количество нулевых значений в колонке StreamingMovies: 0\n",
      "Количество нулевых значений в колонке Contract: 0\n",
      "Количество нулевых значений в колонке PaperlessBilling: 0\n",
      "Количество нулевых значений в колонке PaymentMethod: 0\n",
      "Количество нулевых значений в колонке MonthlyCharges: 0\n",
      "Количество нулевых значений в колонке TotalCharges: 11\n",
      "Количество нулевых значений в колонке Churn: 0\n"
     ]
    }
   ],
   "source": [
    "### Ваш код здесь ###\n",
    "for i in df1.columns:\n",
    "    print(f'Количество нулевых значений в колонке {i}: {df1.filter(col(i).isNull()).count()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.filter(col(\"TotalCharges\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7032"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'SeniorCitizen',\n",
       " 'Partner',\n",
       " 'Dependents',\n",
       " 'tenure',\n",
       " 'PhoneService',\n",
       " 'MultipleLines',\n",
       " 'InternetService',\n",
       " 'OnlineSecurity',\n",
       " 'OnlineBackup',\n",
       " 'DeviceProtection',\n",
       " 'TechSupport',\n",
       " 'StreamingTV',\n",
       " 'StreamingMovies',\n",
       " 'Contract',\n",
       " 'PaperlessBilling',\n",
       " 'PaymentMethod',\n",
       " 'MonthlyCharges',\n",
       " 'TotalCharges',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMRcAbphNBEP"
   },
   "source": [
    "# 6. Feature-инжиниринг\n",
    "Добавьте в датафрейм одну или несколько новых фичей. Удалите колонки, которые, как вам кажется, нужно убрать из фичей. Обоснуйте свои решения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "6SZ_rocx79oY"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "### Ваш код здесь ### \n",
    "# Выше убрал колнку с ID, добавление новых фичей считаю не целесообразным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mX-4GEdHTZWd"
   },
   "source": [
    "# 7. Векторизация фичей\n",
    "Подготовьте данные к обучению:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5cNxN2qq3SL"
   },
   "source": [
    "1. Преобразуйте текстовые колонки в числа, используя StringIndexer.\n",
    "Удалите столбцы со старыми (непреобразованными) признаками. Выведите на экран структуру получившегося датафрейма. Не забывайте о столбце Churn. Хоть он и выступает в задаче как таргет, он имеет текстовый тип, поэтому тоже должен быть закодирован числовыми значениями.\n",
    "\n",
    "Чтобы использовать StringIndexer для всех категориальных признаков сразу, а не для каждого отдельно, можно применить сущность pipeline.\n",
    "\n",
    "**Пример кода:**\n",
    "\n",
    "##### #Задаём список текстовых колонок: \n",
    "text_columns = [\"text_col_1\", \"text_col_2\", \"text_col_3\"]\n",
    "\n",
    "##### #Задаём список StringIndexer'ов — сущностей, каждая из которых будет кодировать одну текстовую колонку числами. Имена преобразованных колонок будут заканчиваться на _index:\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\",).fit(<ваш датасет>) for column in text_columns]\n",
    "\n",
    "##### #Создаём Pipeline из StringIndexer'ов:\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "\n",
    "##### #Скармливаем нашему pipeline датафрейм, удаляя старые колонки:\n",
    "new_dataframe = pipeline.fit(<ваш датасет>).transform(<ваш датасет>).drop(*text_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "0XQ3pjdAcBIL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: double (nullable = true)\n",
      " |-- gender_index: double (nullable = false)\n",
      " |-- Partner_index: double (nullable = false)\n",
      " |-- Dependents_index: double (nullable = false)\n",
      " |-- PhoneService_index: double (nullable = false)\n",
      " |-- MultipleLines_index: double (nullable = false)\n",
      " |-- InternetService_index: double (nullable = false)\n",
      " |-- OnlineSecurity_index: double (nullable = false)\n",
      " |-- OnlineBackup_index: double (nullable = false)\n",
      " |-- DeviceProtection_index: double (nullable = false)\n",
      " |-- TechSupport_index: double (nullable = false)\n",
      " |-- StreamingTV_index: double (nullable = false)\n",
      " |-- StreamingMovies_index: double (nullable = false)\n",
      " |-- Contract_index: double (nullable = false)\n",
      " |-- PaperlessBilling_index: double (nullable = false)\n",
      " |-- PaymentMethod_index: double (nullable = false)\n",
      " |-- Churn_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import mllib\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#список колонок с текстовым типом\n",
    "text_cols = [\"gender\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \\\n",
    "                \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \\\n",
    "                \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"Churn\"]\n",
    "\n",
    "\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df2) for column in text_cols]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df3 = pipeline.fit(df2).transform(df2).drop(*text_cols)\n",
    "df3.printSchema()\n",
    "### Ваш код здесь ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SeniorCitizen',\n",
       " 'tenure',\n",
       " 'MonthlyCharges',\n",
       " 'TotalCharges',\n",
       " 'gender_index',\n",
       " 'Partner_index',\n",
       " 'Dependents_index',\n",
       " 'PhoneService_index',\n",
       " 'MultipleLines_index',\n",
       " 'InternetService_index',\n",
       " 'OnlineSecurity_index',\n",
       " 'OnlineBackup_index',\n",
       " 'DeviceProtection_index',\n",
       " 'TechSupport_index',\n",
       " 'StreamingTV_index',\n",
       " 'StreamingMovies_index',\n",
       " 'Contract_index',\n",
       " 'PaperlessBilling_index',\n",
       " 'PaymentMethod_index',\n",
       " 'Churn_index']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEqoj7IcmB5H"
   },
   "source": [
    "2. Векторизуйте категориальные признаки, используя OneHotEncoder.\n",
    "Удалите столбцы со старыми (непреобразованными) признаками. \n",
    "Выведите на экран структуру получившегося после преобразований датафрейма.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "Gihv_Q00TkOD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: double (nullable = true)\n",
      " |-- Churn_index: double (nullable = false)\n",
      " |-- gender: vector (nullable = true)\n",
      " |-- Partner: vector (nullable = true)\n",
      " |-- Dependents: vector (nullable = true)\n",
      " |-- PhoneService: vector (nullable = true)\n",
      " |-- MultipleLines: vector (nullable = true)\n",
      " |-- InternetService: vector (nullable = true)\n",
      " |-- OnlineSecurity: vector (nullable = true)\n",
      " |-- OnlineBackup: vector (nullable = true)\n",
      " |-- DeviceProtection: vector (nullable = true)\n",
      " |-- TechSupport: vector (nullable = true)\n",
      " |-- StreamingTV: vector (nullable = true)\n",
      " |-- StreamingMovies: vector (nullable = true)\n",
      " |-- Contract: vector (nullable = true)\n",
      " |-- PaperlessBilling: vector (nullable = true)\n",
      " |-- PaymentMethod: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "#список категориальных колонок\n",
    "features_inp  = [\n",
    " 'gender_index',\n",
    " 'Partner_index',\n",
    " 'Dependents_index',\n",
    " 'PhoneService_index',\n",
    " 'MultipleLines_index',\n",
    " 'InternetService_index',\n",
    " 'OnlineSecurity_index',\n",
    " 'OnlineBackup_index',\n",
    " 'DeviceProtection_index',\n",
    " 'TechSupport_index',\n",
    " 'StreamingTV_index',\n",
    " 'StreamingMovies_index',\n",
    " 'Contract_index',\n",
    " 'PaperlessBilling_index',\n",
    " 'PaymentMethod_index']\n",
    "indexers = [OneHotEncoder(inputCol=column, outputCol=column[:-6]).fit(df3) for column in features_inp]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df4 = pipeline.fit(df3).transform(df3).drop(*features_inp)\n",
    "df4.printSchema()\n",
    "\n",
    "### Ваш код здесь ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WscurK1IoJ8w"
   },
   "source": [
    "3. Объедините колонки фичей в один вектор, используя VectorAssembler.\n",
    "Удалите столбцы со старыми (непреобразованными) признаками. \n",
    "Выведите на экран первые несколько строк и структуру получившегося датафрейма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "wOm-Te9bYMia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------------------------------------------------------------------------------------------+\n",
      "|Churn_index|                                                                                                  features_vec|\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------+\n",
      "|        0.0|    (30,[1,2,3,6,11,12,15,16,18,20,22,24,26,27],[1.0,29.85,29.85,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|        0.0|(30,[1,2,3,4,5,6,7,8,11,13,14,17,18,20,22,28],[34.0,56.95,1889.5,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1....|\n",
      "|        1.0|(30,[1,2,3,4,5,6,7,8,11,13,15,16,18,20,22,24,26,28],[2.0,53.85,108.15,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1...|\n",
      "|        0.0|    (30,[1,2,3,4,5,6,11,13,14,17,19,20,22,29],[45.0,42.3,1840.75,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|        1.0|(30,[1,2,3,5,6,7,8,10,12,14,16,18,20,22,24,26,27],[2.0,70.7,151.65,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,...|\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Churn_index: double (nullable = false)\n",
      " |-- features_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "features = list(df4.drop(\"Churn_index\").columns) #Список колонок фичей\n",
    "target = \"Churn_index\"\n",
    "vectorizer = VectorAssembler(inputCols = features, outputCol = \"features_vec\") \n",
    "vectorised = vectorizer.transform(df4).drop(*features)\n",
    "vectorised.show(5,110)\n",
    "vectorised.printSchema()\n",
    "### Ваш код здесь ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk_1ZJjDqm02"
   },
   "source": [
    "# 8. Создание и обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMsi-JYVqvkX"
   },
   "source": [
    "1. Создайте модель — логистическую регрессию (используя LogisticRegression). В качестве параметров класса LogisticRegression укажите колонку фичей (параметр featuresCol), колонку-таргет (параметр labelCol) из датафрейма и имя колонки, в которую будут записываться предсказания (параметр predictionCol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "siYFALNhq_9F"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "### Ваш код здесь ###\n",
    "model = LogisticRegression(featuresCol = 'features_vec', labelCol = target, predictionCol='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCG0euR8v33u"
   },
   "source": [
    "2. Разделите датафрейм на обучающую и тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "g9PoHvGqvvET"
   },
   "outputs": [],
   "source": [
    "\n",
    "### Ваш код здесь ###\n",
    "train, test = vectorised.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyGEK9T_wGWd"
   },
   "source": [
    "3. Создайте объекты — сетки гиперпараметров для каждой модели, используя ParamGridBuilder. Так же, как и в ноутбуке из последнего видео, в сетку гиперпараметров можно добавить значения параметров regParam и elasticNetParam.\n",
    "\n",
    "Вы можете ознакомиться [с документацией объекта LogisticRegression в PySpark](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html) и добавить в сетку больше параметров.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "LsRwAy1TwDzF"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "### Ваш код здесь ###\n",
    "grid = ParamGridBuilder().addGrid(model.regParam, [0.5, 5]).addGrid(model.elasticNetParam, [0.01, 0.1]).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg0lwkdoJjaQ"
   },
   "source": [
    "4. Создайте объект evaluator, который будет отвечать за метрику качества при обучении. Для этого используйте класс BinaryClassificationEvaluator со следующими параметрами: rawPredictionCol — колонка с предсказаниями, labelCol — колонка с таргетом.\n",
    "\n",
    "У вас, возможно, возник вопрос, какую метрику качества берёт по умолчанию BinaryClassificationEvaluator. По умолчанию BinaryClassificationEvaluator будет рассчитывать areaUnderROC. Это метрика оценки площади под кривой ROC (Receiver Operating Characteristic), которая служит графической интерпретацией производительности модели. Эта метрика качества находится в пределах от 0 до 1. Чем выше метрика, тем более качественные предсказания делает модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "RrGPelUqMeoz"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "  \n",
    "### Ваш код здесь ###\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='predict', labelCol=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7urf-OeRMTdC"
   },
   "source": [
    "5. Создайте объект CrossValidator, в качестве параметров укажите уже созданные вами модель, сетку гиперпараметров и evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "CtmsG_rxSOOr"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "### Ваш код здесь ###\n",
    "cv = CrossValidator(estimator = model, estimatorParamMaps = grid, evaluator = evaluator) #Создаём объект cv — экземпляр CrossValidator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmpv70TjZByl"
   },
   "source": [
    "6. Запустите обучение модели на тестовой выборке. Сохраните обученную модель в новую переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "uUxWevjEYOvX"
   },
   "outputs": [],
   "source": [
    "cv_model = cv.fit(train)\n",
    "### Ваш код здесь ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaIi8In5Zykn"
   },
   "source": [
    "# 9. Выбор лучшей модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29eo5S_KZL4k"
   },
   "source": [
    "1. Выберите лучшую модель, сохраните её в отдельную переменную, отобразите её параметры.\n",
    "\n",
    "Вывод параметров модели в PySpark можно сделать, используя метод extractParamMap()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "kJK-IrWLZbL-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='CrossValidatorModel_679ba67db977', name='seed', doc='random seed.'): 3370440681401766509,\n",
       " Param(parent='CrossValidatorModel_679ba67db977', name='numFolds', doc='number of folds for cross validation'): 3,\n",
       " Param(parent='CrossValidatorModel_679ba67db977', name='foldCol', doc=\"Param for the column name of user specified fold number. Once this is specified, :py:class:`CrossValidator` won't do random k-fold split. Note that this column should be integer type with range [0, numFolds) and Spark will throw exception on out-of-range fold numbers.\"): '',\n",
       " Param(parent='CrossValidatorModel_679ba67db977', name='estimator', doc='estimator to be cross-validated'): LogisticRegression_f76f7f6ec9e4,\n",
       " Param(parent='CrossValidatorModel_679ba67db977', name='estimatorParamMaps', doc='estimator param maps'): [{Param(parent='LogisticRegression_f76f7f6ec9e4', name='regParam', doc='regularization parameter (>= 0).'): 0.5,\n",
       "   Param(parent='LogisticRegression_f76f7f6ec9e4', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.01},\n",
       "  {Param(parent='LogisticRegression_f76f7f6ec9e4', name='regParam', doc='regularization parameter (>= 0).'): 0.5,\n",
       "   Param(parent='LogisticRegression_f76f7f6ec9e4', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1},\n",
       "  {Param(parent='LogisticRegression_f76f7f6ec9e4', name='regParam', doc='regularization parameter (>= 0).'): 5.0,\n",
       "   Param(parent='LogisticRegression_f76f7f6ec9e4', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.01},\n",
       "  {Param(parent='LogisticRegression_f76f7f6ec9e4', name='regParam', doc='regularization parameter (>= 0).'): 5.0,\n",
       "   Param(parent='LogisticRegression_f76f7f6ec9e4', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1}],\n",
       " Param(parent='CrossValidatorModel_679ba67db977', name='evaluator', doc='evaluator used to select hyper-parameters that maximize the validator metric'): BinaryClassificationEvaluator_2b2726c27cae}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### Ваш код здесь ###\n",
    "cv_model.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BIe8gqugBds"
   },
   "source": [
    "2. Запустите лучшую модель в режиме предсказания на тренировочной выборке. Сохраните предсказания в отдельную переменную. Выведите первые несколько строк датафрейма с предсказаниями на экран.\n",
    "\n",
    "Запуск модели в режиме предсказания выполняется при помощи метода .transform(<тестовая выборка>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "_K1IrbNzd1FX"
   },
   "outputs": [],
   "source": [
    "\n",
    "### Ваш код здесь ###\n",
    "predict = cv_model.bestModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC41fZbahGCB"
   },
   "source": [
    "3. Получите метрику качества модели. Для этого примените к объекту evaluator метод .evaluate(<ваш датафрейм с предсказаниями>).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "CBzDaleQhejy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5705123339658443"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### Ваш код здесь ###\n",
    "evaluator.evaluate(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nieC-XnAl-4m"
   },
   "source": [
    "# 10. Обратная связь\n",
    "Вы ознакомились с возможностями двух мощных библиотек: PySpark SQL для предобработки данных и PySpark ML для машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axLO-xV3pJx2"
   },
   "source": [
    "Поделитесь впечатлениями от работы с новыми библиотеками. В чём они более удобны, чем уже знакомые вам Pandas и Sklearn, а в чём нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Безусловным преимуществом новых бибилитек является скорость их работы, хотя на учебном датасете это не получилось прочувствовать. В виду уже некоторого опыта работы в Pandas и Sklearn они кажутся более удобными и логичными, но может быть это только по началу. Определенно в pandas работать удобней в виду более наглядной визуализации информации, но в свое время казалось, что excel удобней pandas и, хотя excel я все еще использую в работе, львиную чать работы по обработке информации с большими датафреймами сейчас выполняю в pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9TYdK5gl9P4"
   },
   "source": [
    "# Как отправить работу на проверку\n",
    "\n",
    "Загрузите файл с заданиями, откройте его через Jupyter Notebook в Google Colab. Скачайте файл с датасетом и загрузите его в Colab. Выполните задачи, сохраните изменения: воспользуйтесь опцией Save and Checkpoint из вкладки меню File или кнопкой Save and Checkpoint на панели инструментов. Отправьте через форму ниже итоговый файл Jupyter Notebook (.ipynb) или ссылку на него."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
